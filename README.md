# âš™ï¸ ZaubaCorp Data Pipeline (Web Scraping Â· Airflow Â· Mongodb Â· API)

---

## ğŸ“˜ Overview

**ZaubaCorp Data Pipeline** is an **end-to-end data engineering project** built entirely in **Python**, designed to scrape company profile data from **ZaubaCorp**, process it through a structured **ETL pipeline**, and expose the cleaned data via a **REST API**.

The pipeline is orchestrated using **Apache Airflow**, stores data in **MongoDB**, and serves results through a **FastAPI** application. The entire system is containerized using **Docker Compose** for easy setup and reproducibility.

It enables:
1. **Automated Web Scraping** of company profiles
2. **Raw & Cleaned Data Storage** in MongoDB
3. **Scheduled ETL Orchestration** using Airflow
4. **API-based Data Access** using CIN

---

## ğŸ—ï¸ Solution Architecture
```
ZaubaCorp Website
â”‚
â–¼
Selenium Scraper
â”‚
â–¼
MongoDB (companies_raw)
â”‚
â–¼
Data Cleaning & Transformation
â”‚
â–¼
MongoDB (companies_cleaned)
â”‚
â–¼
FastAPI REST Service
â”‚
â–¼
Client / Postman / curl
```

All steps are executed and monitored through an **Apache Airflow DAG**.

---

## âš™ï¸ Technologies Used

| Category | Technologies |
|---|---|
| **Language** | Python |
| **Web Scraping** | Selenium |
| **Workflow Orchestration** | Apache Airflow |
| **Database** | MongoDB Atlas |
| **API Framework** | FastAPI |
| **Containerization** | Docker, Docker Compose |
| **Libraries** | Pydantic, PyMongo, Uvicorn |

---

## ğŸ”„ Pipeline Breakdown

### ğŸ”¹ Step 1: Web Scraping

- Selenium scrapes **100 company profiles** from ZaubaCorp
- Extracted fields include:
  - CIN
  - Company Name
  - Company Status
  - ROC
  - Registration Number
  - Company Category & Sub-category
  - Class of Company
  - Date of Incorporation
  - Authorized & Paid-up Capital

Minimum mandatory fields:
- CIN
- Company Name
- Incorporation Date
- Company Status

---

### ğŸ”¹ Step 2: Store Raw Data

- Scraped data is stored as-is in MongoDB
- Collection name:
```
companies_raw
```

This ensures traceability and allows reprocessing if needed.

---

### ğŸ”¹ Step 3: Data Cleaning & Transformation

The cleaning script performs:
- Removal of extra spaces and invalid characters
- Standardization of key names
- Date normalization to ISO format
- Numeric type conversion
- Duplicate removal based on CIN

Cleaned output is stored in:
```
companies_cleaned
```

---

### ğŸ”¹ Step 4: Airflow Orchestration

All steps are orchestrated using a single **Airflow DAG** with clear task separation:
```
scrape_data â†’ save_raw â†’ clean_data â†’ save_cleaned
```

Features:
- End-to-end execution
- Task-level logging
- Retry support
- Docker-based Airflow deployment

---

### ğŸ”¹ Step 5: REST API for Data Access

A **FastAPI** service exposes cleaned company data using CIN as input.

#### API Endpoint
```
POST /company
```

#### Request Body
```json
{
  "cin": "L12345MH2010PLC123456"
}
```

#### Sample Response
```json
{
  "cin": "L12345MH2010PLC123456",
  "name": "ABC PRIVATE LIMITED",
  "company_status": "Active",
  "date_of_incorporation": "2010-05-12T00:00:00",
  "roc": "ROC-Mumbai"
}
```

If CIN is not found, the API returns a proper HTTP error response.

---

## ğŸš€ Setup & Execution

### Prerequisites

- Python 3.9+
- Docker & Docker Compose
- Google Chrome & ChromeDriver
- MongoDB Atlas account

---

### 1ï¸âƒ£ Clone Repository
```bash
git clone <your-github-repo-url>
cd ZINTLR-DATA-PIPELINE
```

---

### 2ï¸âƒ£ Configure Environment Variables

Create a `.env` file and set:
```env
MONGODB_URI=mongodb+srv://<username>:<password>@cluster.mongodb.net/
MONGODB_DB=zintlr
```

---

### 3ï¸âƒ£ Start Services
```bash
docker-compose up --build
```

---

### 4ï¸âƒ£ Run Airflow DAG

- Open Airflow UI: `http://localhost:8080`
- Login:
  - Username: `airflow`
  - Password: `airflow`
- Enable and trigger the `zintlr_scraper` DAG
- Monitor logs for successful execution

---

### 5ï¸âƒ£ Verify MongoDB Data

Check the following collections:
- `companies_raw`
- `companies_cleaned`

Using MongoDB Atlas UI or MongoDB Compass.

---

### 6ï¸âƒ£ Run & Test API

- API Base URL: `http://localhost:8000`
- Health Check:
```
GET /
```

- Fetch company by CIN:
```
POST /company
```

---

## ğŸ§  Key Features

âœ… End-to-end automated ETL pipeline  
âœ… Selenium-based controlled scraping  
âœ… Airflow DAG orchestration  
âœ… Raw and cleaned data separation  
âœ… CIN-based deduplication  
âœ… REST API for real-time access  
âœ… Fully Dockerized setup  

---

## âš”ï¸ Challenges & Solutions

| Challenge | Solution |
|---|---|
| Dynamic website structure | Explicit waits and stable selectors |
| Duplicate company records | CIN-based deduplication |
| Data inconsistency | Centralized cleaning logic |
| Local setup complexity | Docker Compose orchestration |

---

## ğŸ“‚ Screenshots

All required screenshots are available in the `/screenshots` folder:

- Airflow DAG view
- Successful DAG logs
- MongoDB collections
- API response (Postman/curl)

---

## ğŸ Conclusion

This project demonstrates a complete **production-style data pipeline** using Python, Apache Airflow, MongoDB, and FastAPI. It highlights best practices in scraping, data cleaning, orchestration, and API development while maintaining modular and scalable design.

---

## ğŸ‘¨â€ğŸ’» Author

**A. Yashwanth**  
Aspiring Data Engineer | Python & Data Engineering Enthusiast

ğŸ”— [www.linkedin.com/in/yashwantharavanti](http://www.linkedin.com/in/yashwantharavanti)






```text
ZINTLR-DATA-PIPELINE/
â”‚
â”œâ”€â”€ airflow/
â”‚   â”œâ”€â”€ dags/
â”‚   â”‚   â””â”€â”€ zintlr_scraper.py
â”‚   â”œâ”€â”€ logs/
â”‚   â””â”€â”€ plugins/
â”‚
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ db.py
â”‚   â”œâ”€â”€ schemas.py
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ scraper.py
â”‚   â”œâ”€â”€ cleaner.py
â”‚   â””â”€â”€ links.txt
â”‚
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ screenshots/
